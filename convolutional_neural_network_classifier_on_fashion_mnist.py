# -*- coding: utf-8 -*-
"""Convolutional Neural Network Classifier on Fashion MNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lymhx9ZooxTWN_9b9m0ImyKwm9a3_Py1

Import PyTorch
"""

import torch
import torchvision
import torchvision.transforms as transforms

torch.cuda.is_available()

device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
device

transform = transforms.ToTensor()
batch_size = 8    #This means the GPU is going to handle 8 different examples at a time

trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)

testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)

classes = ('T-shirt/top', 'Trouser/pants', 'Pullover shirt', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')

train_iter = iter(trainset)

image, label = next(train_iter)

image.shape, label

#The label is 9, it should be a class of an Ankle boot

torch.min(image).item(), torch.max(image).item()

import matplotlib.pyplot as plt
import numpy as np

np_img = image.numpy()
print(classes[label])
plt.imshow(np_img.reshape((28, 28, 1)))

len(trainset), len(testset)

trainset, valset = torch.utils.data.random_split(trainset, [50000, 10000])
len(trainset), len(valset), len(testset)

print(f'Number of batches in the training set: {len(trainset) / batch_size}')
print(f'Number of batches in the validation set: {len(valset) / batch_size}')

type(trainset)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)

valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)

testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)

import torch.nn as nn
import torch.nn.functional as F

class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()

        self.conv1 = nn.Conv2d(in_channels=1, out_channels=256, kernel_size=3)
        self.pool1 = nn.MaxPool2d(2, 2)

        self.conv2 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3)
        self.pool2 = nn.MaxPool2d(2, 2)

        self.conv3 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=2)
        self.pool3 = nn.MaxPool2d(2, 2)

        self.flatten = nn.Flatten()

        self.fc1 = nn.Linear(in_features=4096, out_features=1024)
        self.drop1 = nn.Dropout(0.3)

        self.fc2 = nn.Linear(in_features=1024, out_features=1024)
        self.drop2 = nn.Dropout(0.3)

        self.out = nn.Linear(in_features=1024, out_features=10)


    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool1(x)

        x = F.relu(self.conv2(x))
        x = self.pool2(x)

        x = F.relu(self.conv3(x))
        x = self.pool3(x)

        x = self.flatten(x)

        x = F.relu(self.fc1(x))
        x = self.drop1(x)

        x = F.relu(self.fc2(x))
        x = self.drop2(x)

        x = self.out(x)

        return x

net = NeuralNetwork()
net.to(device)

for i, data in enumerate(trainloader):
  inputs, labels = data[0].to(device), data[1].to(device)
  print(f'input shape: {inputs.shape}')
  print(f'after network shape: {net(inputs).shape}')
  break

num_params = 0
for x in net.parameters():
  num_params += len(torch.flatten(x))

print(f'Number of parameters: {num_params:,}')

import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.0001)

def train_one_epoch():
  net.train(True)

  running_loss = 0.0
  running_accuracy = 0.0

  for batch_index, data in enumerate(trainloader):
    inputs, labels = data[0].to(device), data[1].to(device)

    optimizer.zero_grad()

    outputs = net(inputs) #shape: [batch_size, 10]
    correct = torch.sum(labels == torch.argmax(outputs, dim=1)).item()
    running_accuracy += correct / batch_size

    loss = criterion(outputs, labels)
    running_loss += loss.item()
    loss.backward()
    optimizer.step()

    if batch_index % 500 == 499:  #Will print every 500 batches
      avg_loss_across_batches = running_loss / 500
      avg_acc_across_batches = (running_accuracy / 500) * 100
      print(f'Batch {batch_index + 1}    Loss: {avg_loss_across_batches:0.3f}    Accuracy: {avg_acc_across_batches:0.1f}%')

      running_loss = 0.0
      running_accuracy = 0.0

  print()

def validate_one_epoch():
  net.train(False)

  running_loss = 0.0
  running_accuracy = 0.0

  for i in enumerate(valloader):
    inputs, labels = data[0].to(device), data[1].to(device)

    with torch.no_grad():  #Since we are only validating the model, we don't need gradients to train it
      outputs = net(inputs)
      correct = torch.sum(labels == torch.argmax(outputs, dim=1)).item()
      running_accuracy += correct / batch_size

      loss = criterion(outputs, labels) #This output is the average batch loss
      running_loss += loss.item()

  avg_loss_across_batches = running_loss / len(valloader)
  avg_acc_across_batches = (running_accuracy / len(valloader)) * 100

  print(f'Validation Loss: {avg_loss_across_batches:0.3f}     Validation Accuracy: {avg_acc_across_batches:0.1f}%')
  print('********************************************************')
  print()

num_epochs = 10

for epoch_index in range(num_epochs):
  print(f'Epoch: {epoch_index + 1}\n')

  train_one_epoch()
  validate_one_epoch()

print('Training Complete')